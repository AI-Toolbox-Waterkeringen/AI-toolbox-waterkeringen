{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7410fa-ca3f-4e87-a4aa-fd5ac9cf06a2",
   "metadata": {},
   "source": [
    "# Opschotdetectie\n",
    "Deze notebook bevat een voorbeeld van opschotdetectie. Opschot zien we als ongewenste, houtachtige begroeiing op een steenbekleding van een waterkering. De methode om opschot te detecteren is in dit voorbeeld gebaseerd op zero-shot inference met de foundation models GroundingDino en SegmentAnything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df8ab0-e054-4ca3-8684-b911d92a5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes\n",
    "from typing import List, Tuple, Iterable\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import groundingdino.util.inference\n",
    "import groundingdino.util.utils\n",
    "\n",
    "from hydra import initialize, compose\n",
    "\n",
    "with initialize(\"config\", version_base=None):\n",
    "    cfg = compose(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e16db-19c8-49a1-ad89-6828830fb27b",
   "metadata": {},
   "source": [
    "## Voorbeeld foto met opschot\n",
    "We passen de methode toe op een drone foto van een kering met steenbekleding. Op de steenbekleding is duidelijk opschot aanwezig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206365b2-a605-406c-a0d1-3c00d5ed9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = 'images/test.jpeg'\n",
    "\n",
    "# Load image\n",
    "image_pil = Image.open(imgPath).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "image = transform(image_pil)\n",
    "image_np = np.array(image_pil)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "ax.imshow(image_pil)\n",
    "ax.axis(\"off\")\n",
    "fig.tight_layout();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e3379-4e7f-48cb-9b4b-ae61e4cdce19",
   "metadata": {},
   "source": [
    "## Foundation models\n",
    "Om opschot te detecteren, moeten we de locatie van opschot bepalen en vervolgens uitknippen. De locatie bepalen we met behulp van GroundingDino, waarna we de precieze locatie uitknippen met SegmentAnything. Om dit te doen moeten we eerst de twee modellen inladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94f07-9349-4ec6-898f-56bddb9a81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Grounding Dino Model\n",
    "dino_model = groundingdino.util.inference.load_model(\n",
    "    cfg.GROUNDING_DINO_CONFIG_PATH,\n",
    "    cfg.GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "    device=cfg.DEVICE,\n",
    ")\n",
    "\n",
    "# Load Segment Anything Model (SAM)\n",
    "if cfg.USE_SAM_HQ:\n",
    "    from segment_anything_hq import SamPredictor as SamPredictor_hq\n",
    "    from segment_anything_hq import sam_model_registry as sam_model_registry_hq\n",
    "\n",
    "    sam = sam_model_registry_hq[cfg.SAM_HQ_ENCODER_VERSION](\n",
    "        checkpoint=cfg.SAM_HQ_CHECKPOINT_PATH\n",
    "    ).to(device=cfg.DEVICE)\n",
    "    sam_predictor = SamPredictor_hq(sam)\n",
    "else:\n",
    "    from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "    sam = sam_model_registry[cfg.SAM_ENCODER_VERSION](\n",
    "        checkpoint=cfg.SAM_CHECKPOINT_PATH\n",
    "    ).to(device=cfg.DEVICE)\n",
    "    sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0f47-7f63-4688-be0b-f4079aad6c9a",
   "metadata": {},
   "source": [
    "## Trefwoorden voor GroundingDino om opschot te detecteren\n",
    "GroundingDino detecteert objecten op basis van een of meerdere trefwoorden ('tags'). In dit voorbeeld geven we de trefwoorden 'bush', 'shrub', 'tree' en 'plant' mee aan GroundingDino om opschot te detecteren. GroundingDino produceert op basis van deze trefwoorden bounding boxes in de figuur waar deze trefwoorden worden aangetroffen. Op basis van Non-maximum-Supression worden overlappende bounding boxes tot 1 box samengevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f3b09-9a52-4df8-bc27-42ba12ca491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = [\"bush\", \"shrub\", \"tree\", \"plant\"]\n",
    "tags = \". \".join(tag_list)\n",
    "\n",
    "def get_grounding_output(\n",
    "    model: torch.nn.Module,\n",
    "    image: torch.Tensor,\n",
    "    caption: str,\n",
    "    box_threshold: float,\n",
    "    text_threshold: float,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Process an image and caption through a model to generate grounded outputs,\n",
    "    including filtered bounding boxes and corresponding text phrases.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to process the input data.\n",
    "    - image (torch.Tensor): The image tensor.\n",
    "    - caption (str): The caption string related to the image.\n",
    "    - box_threshold (float): The threshold value to filter the bounding boxes based on confidence scores.\n",
    "    - text_threshold (float): The threshold value to filter the text based on logits.\n",
    "    - device (str, optional): The device type, 'cpu' or 'cuda', where the computation will take place. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "    - tuple:\n",
    "        - filtered_boxes (torch.Tensor): The filtered bounding boxes.\n",
    "        - scores (torch.Tensor): The confidence scores of the phrases.\n",
    "        - pred_phrases (list of str): The predicted phrases associated with the bounding boxes.\n",
    "    \"\"\"\n",
    "    # Prepare caption\n",
    "    caption = caption.lower().strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption += \".\"\n",
    "\n",
    "    # Move model and image to the specified device\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Generate predictions\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                image.unsqueeze(0), captions=[caption]\n",
    "            )  # Ensure image is 4D\n",
    "        logits = outputs[\"pred_logits\"].sigmoid()[0]  # (num_queries, num_classes)\n",
    "        boxes = outputs[\"pred_boxes\"][0]  # (num_queries, 4)\n",
    "\n",
    "        # Filter outputs based on thresholds\n",
    "        max_logits = logits.max(dim=1)[0]\n",
    "        filt_mask = max_logits > box_threshold\n",
    "        logits_filt = logits[filt_mask]\n",
    "        boxes_filt = boxes[filt_mask]\n",
    "\n",
    "        # Prepare phrases and scores\n",
    "        tokenizer = model.tokenizer\n",
    "        tokenized = tokenizer(caption)\n",
    "        pred_phrases, scores = [], []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = groundingdino.util.utils.get_phrases_from_posmap(\n",
    "                logit > text_threshold, tokenized, tokenizer\n",
    "            )\n",
    "            pred_phrases.append(f\"{pred_phrase} ({logit.max().item():.4f})\")\n",
    "            scores.append(logit.max().item())\n",
    "\n",
    "        return boxes_filt, torch.tensor(scores), pred_phrases\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred during model prediction: {e}\")\n",
    "\n",
    "# Find bounding boxes with grounding dino\n",
    "boxes_filt, scores, pred_phrases = get_grounding_output(\n",
    "    dino_model,\n",
    "    image,\n",
    "    tags,\n",
    "    0.35,\n",
    "    0.25,\n",
    "    device=cfg.DEVICE,\n",
    ")\n",
    "boxes_filt =boxes_filt.cpu()\n",
    "\n",
    " # Resize boxes\n",
    "size = image_pil.size\n",
    "H, W = size[1], size[0]\n",
    "for i in range(boxes_filt.size(0)):\n",
    "    boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])\n",
    "    boxes_filt[i][:2] -= boxes_filt[i][2:] / 2\n",
    "    boxes_filt[i][2:] += boxes_filt[i][:2]\n",
    "\n",
    "# use NMS to handle overlapped boxes\n",
    "nms_idx = (\n",
    "    torchvision.ops.nms(boxes_filt, scores, 0.5).numpy().tolist()\n",
    ")\n",
    "if cfg.DO_IOU_MERGE:\n",
    "    boxes_filt_clean = boxes_filt[nms_idx]\n",
    "    pred_phrases_clean = [pred_phrases[idx] for idx in nms_idx]\n",
    "    print(f\"NMS: before {boxes_filt.shape[0]} boxes, after {boxes_filt_clean.shape[0]} boxes\")\n",
    "else:\n",
    "    boxes_filt_clean = boxes_filt\n",
    "    pred_phrases_clean = pred_phrases\n",
    "\n",
    "def show_box(box: Iterable[float], ax: matplotlib.axes.Axes, label: str) -> None:\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - x0, box[3] - y0\n",
    "    rect = plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=\"none\", lw=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        x0,\n",
    "        y0,\n",
    "        label,\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"white\",\n",
    "        fontsize=8,\n",
    "        bbox={\"facecolor\": \"black\", \"alpha\": 0.5},\n",
    "    )\n",
    "    return None\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n",
    "\n",
    "ax = axs[0, 0]\n",
    "ax.imshow(image_np)\n",
    "ax.set_title(\"Origineel\", wrap=True)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axs[0, 1]\n",
    "ax.imshow(image_np)\n",
    "for box, label in zip(boxes_filt_clean, pred_phrases_clean):\n",
    "    show_box(box.numpy(), ax, label)\n",
    "ax.set_title(f\"GroundingDino tags: {tag_list}\", wrap=True)\n",
    "ax.axis(\"off\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811503c1-ba6c-4b87-a4ec-b59a2b7ab85f",
   "metadata": {},
   "source": [
    "## Uitknippen van opschot met SegmentAnything\n",
    "Op basis van de zojuist gedetecteerde objecten, kunnen met SegmentAnything de contouren gedetecteerd worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4738b3b-0cd6-40bd-a731-6e62b1a12973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(\n",
    "    mask: np.ndarray, ax: matplotlib.axes.Axes, random_color: bool = False\n",
    ") -> None:\n",
    "    if random_color:\n",
    "        color = np.random.rand(3)  # Generates three random floats between 0 and 1\n",
    "        color = np.append(color, 0.6)  # Add alpha for transparency\n",
    "    else:\n",
    "        color = np.array(\n",
    "            [30 / 255, 144 / 255, 255 / 255, 0.6]\n",
    "        )  # Deep sky blue with transparency\n",
    "\n",
    "    h, w = mask.shape\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    return None\n",
    "\n",
    "# Segment objects with SAM\n",
    "sam_predictor.set_image(image_np)\n",
    "transformed_boxes = sam_predictor.transform.apply_boxes_torch(\n",
    "    boxes_filt_clean, image_np.shape[:2]\n",
    ").to(cfg.DEVICE)\n",
    "masks, _, _ = sam_predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes.to(cfg.DEVICE),\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "for cat_title, mask in zip(pred_phrases_clean, masks):\n",
    "    mask = mask.cpu().numpy()\n",
    "\n",
    "# Setup figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n",
    "\n",
    "ax = axs[0, 0]\n",
    "ax.imshow(image_np)\n",
    "ax.set_title(\"Origineel\", wrap=True)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axs[0, 1]\n",
    "ax.imshow(image_np)\n",
    "for mask in masks:\n",
    "    show_mask(mask[0,...].cpu().numpy(), ax, random_color=False)\n",
    "for box, label in zip(boxes_filt_clean, pred_phrases_clean):\n",
    "    show_box(box.numpy(), ax, label)\n",
    "ax.set_title(f\"GroundingDino + SegmentAnything, tags: {tag_list}\", wrap=True)\n",
    "ax.axis(\"off\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
