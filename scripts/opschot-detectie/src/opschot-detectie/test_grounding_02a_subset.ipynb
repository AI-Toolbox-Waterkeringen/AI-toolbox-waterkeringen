{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# COCO tools\n",
    "import pycocotools.mask\n",
    "import rasterio.features\n",
    "import shapely.geometry\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import utils\n",
    "\n",
    "# # Recognize Anything Model & Tag2Text\n",
    "# from ram.models import ram_plus\n",
    "# from ram import inference_ram\n",
    "# import torchvision.transforms as TS\n",
    "# Grounding Dino\n",
    "from groundingdino.util.inference import load_model\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# Segment anything\n",
    "from segment_anything_hq import (\n",
    "    SamPredictor as SamPredictor_hq,\n",
    ")\n",
    "from segment_anything_hq import (\n",
    "    sam_model_registry as sam_model_registry_hq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image(image_path):\n",
    "#     # load image\n",
    "#     image_pil = PIL.Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "#     transform = T.Compose(\n",
    "#         [\n",
    "#             T.RandomResize([800], max_size=1333),\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#         ]\n",
    "#     )\n",
    "#     image, _ = transform(image_pil, None)  # 3, h, w\n",
    "#     return image_pil, image\n",
    "\n",
    "# def get_grounding_output(model, image, caption, box_threshold, text_threshold,device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     caption = caption.lower()\n",
    "#     caption = caption.strip()\n",
    "#     if not caption.endswith(\".\"):\n",
    "#         caption = caption + \".\"\n",
    "#     model = model.to(device)\n",
    "#     image = image.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(image[None], captions=[caption])\n",
    "#     logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]  # (nq, 256)\n",
    "#     boxes = outputs[\"pred_boxes\"].cpu()[0]  # (nq, 4)\n",
    "#     logits.shape[0]\n",
    "\n",
    "#     # filter output\n",
    "#     logits_filt = logits.clone()\n",
    "#     boxes_filt = boxes.clone()\n",
    "#     filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "#     logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "#     boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "#     logits_filt.shape[0]\n",
    "\n",
    "#     # get phrase\n",
    "#     tokenlizer = model.tokenizer\n",
    "#     tokenized = tokenlizer(caption)\n",
    "#     # build pred\n",
    "#     pred_phrases = []\n",
    "#     scores = []\n",
    "#     for logit, box in zip(logits_filt, boxes_filt):\n",
    "#         pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "#         pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "#         scores.append(logit.max().item())\n",
    "#     return boxes_filt, torch.Tensor(scores), pred_phrases\n",
    "\n",
    "\n",
    "# def show_mask(mask, ax, random_color=False):\n",
    "#     if random_color:\n",
    "#         color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "#     else:\n",
    "#         color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "#     h, w = mask.shape[-2:]\n",
    "#     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "#     ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "# def show_box(box, ax, label):\n",
    "#     x0, y0 = box[0], box[1]\n",
    "#     w, h = box[2] - box[0], box[3] - box[1]\n",
    "#     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "#     ax.text(x0, y0, label)\n",
    "\n",
    "\n",
    "# def save_mask_data(output_dir, tags_chinese, mask_list, box_list, label_list):\n",
    "#     value = 0  # 0 for background\n",
    "\n",
    "#     mask_img = torch.zeros(mask_list.shape[-2:])\n",
    "#     for idx, mask in enumerate(mask_list):\n",
    "#         mask_img[mask.cpu().numpy()[0] == True] = value + idx + 1\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(mask_img.numpy())\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(os.path.join(output_dir, 'mask.jpg'), bbox_inches=\"tight\", dpi=300, pad_inches=0.0)\n",
    "\n",
    "#     json_data = {\n",
    "#         'tags_chinese': tags_chinese,\n",
    "#         'mask':[{\n",
    "#             'value': value,\n",
    "#             'label': 'background'\n",
    "#         }]\n",
    "#     }\n",
    "#     for label, box in zip(label_list, box_list):\n",
    "#         value += 1\n",
    "#         name, logit = label.split('(')\n",
    "#         logit = logit[:-1] # the last is ')'\n",
    "#         json_data['mask'].append({\n",
    "#             'value': value,\n",
    "#             'label': name,\n",
    "#             'logit': float(logit),\n",
    "#             'box': box.numpy().tolist(),\n",
    "#         })\n",
    "#     with open(os.path.join(output_dir, 'label.json'), 'w') as f:\n",
    "#         json.dump(json_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dino thresholds\n",
    "# DINO_BOX_THRESHOLD = 0.2\n",
    "# DINO_TEXT_THRESHOLD = 0.2\n",
    "\n",
    "# # Boxes merge\n",
    "# DO_IOU_MERGE = True\n",
    "# IOU_THRESHOLD = 0.5\n",
    "\n",
    "# # tiffbounds\n",
    "# path_tifftiles = Path(\"data/tile_dataset\")\n",
    "\n",
    "# DEVICE = \"cuda:1\"\n",
    "\n",
    "# fixed_tags = True\n",
    "# RAM_CHECKPOINT = \".//ram_weights/ram_plus_swin_large_14m.pth\"\n",
    "# RAM_MODELTYPE = \"swin_l\"\n",
    "\n",
    "# GROUNDING_DINO_CONFIG_PATH = \"config/GroundingDINO_SwinB_cfg.py\"\n",
    "# GROUNDING_DINO_CHECKPOINT_PATH = \"data/dino_weights/groundingdino_swinb_cogcoor.pth\"\n",
    "\n",
    "\n",
    "# USE_SAM_HQ = False\n",
    "# SAM_ENCODER_VERSION = \"vit_h\"\n",
    "# SAM_CHECKPOINT_PATH = \"data/sam_weights/sam_vit_h_4b8939.pth\"\n",
    "# SAM_HQ_ENCODER_VERSION = \"vit_h\"\n",
    "# SAM_HQ_CHECKPOINT_PATH = \"data/sam_weights/sam_hq_vit_h.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Recognize Anything Model (RAM)\n",
    "# normalize = TS.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# transform = TS.Compose([TS.Resize((384, 384)), TS.ToTensor(), normalize])\n",
    "# ram_model = ram_plus(pretrained=RAM_CHECKPOINT, image_size=384, vit=RAM_MODELTYPE)\n",
    "# pd.DataFrame(dict(ram_class=ram_model.tag_list, ram_threshold=ram_model.class_threshold)).to_csv(\"RAM_tag_list.csv\")\n",
    "# # ram_model.class_threshold = torch.ones(ram_model.num_class) * RAM_THRESHOLD\n",
    "\n",
    "# ram_model.eval()\n",
    "# ram_model = ram_model.to(DEVICE)\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "\n",
    "with initialize(\"config\", version_base=None):\n",
    "    cfg = compose(\"config.yaml\")\n",
    "\n",
    "print(cfg)\n",
    "# Load grounding dino model\n",
    "dino_model = load_model(\n",
    "    cfg.GROUNDING_DINO_CONFIG_PATH,\n",
    "    cfg.GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "    device=cfg.DEVICE,\n",
    ")\n",
    "\n",
    "# Segment Anything Model (SAM)\n",
    "\n",
    "if cfg.USE_SAM_HQ:\n",
    "    print(\"Initialize SAM-HQ Predictor\")\n",
    "    sam = sam_model_registry_hq[cfg.SAM_HQ_ENCODER_VERSION](\n",
    "        checkpoint=cfg.SAM_HQ_CHECKPOINT_PATH\n",
    "    ).to(device=cfg.DEVICE)\n",
    "    sam_predictor = SamPredictor_hq(sam)\n",
    "else:\n",
    "    sam = sam_model_registry[cfg.SAM_ENCODER_VERSION](\n",
    "        checkpoint=cfg.SAM_CHECKPOINT_PATH\n",
    "    ).to(device=\"cuda:1\")\n",
    "    sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tile_bounds(root_tilepath, concat=True):\n",
    "    \"\"\"\n",
    "    Recursively search through directories starting from root_tilepath to find and optionally concatenate\n",
    "    geospatial data files (.gpkg) that include 'cells_intersect' in their filename.\n",
    "\n",
    "    Parameters:\n",
    "    - root_tilepath (str or pathlib.Path): The root directory path where the search for tile files begins.\n",
    "    - concat (bool, optional): If True, concatenates all found geospatial data into a single DataFrame.\n",
    "                               If False, returns a list of DataFrames. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame or list of geopandas.GeoDataFrame: The concatenated DataFrame of all files if `concat=True`,\n",
    "      or a list of DataFrames for each file if `concat=False`.\n",
    "\n",
    "    \"\"\"\n",
    "    cell_files = []\n",
    "    for p in Path(str(root_tilepath)).iterdir():\n",
    "        if p.is_dir():\n",
    "            cell_files += find_tile_bounds(p, concat=False)\n",
    "        elif p.is_file() and p.suffix == \".gpkg\" and \"tiles_intersect\" in p.stem:\n",
    "            df = gpd.read_file(p)\n",
    "            df[\"name\"] = p.stem.replace(\"_tiles_intersects\", \"\")\n",
    "            cell_files.append(df.copy())\n",
    "\n",
    "    if concat:\n",
    "        cell_files = pd.concat(cell_files)\n",
    "\n",
    "    return cell_files\n",
    "\n",
    "\n",
    "# tilebounds\n",
    "df_tilebounds = find_tile_bounds(Path(cfg.disk_path) / \"tile_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tilebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(cfg.disk_path) / \"output\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir()\n",
    "else:\n",
    "    print(f\"Directory {out_dir} already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisely_path = Path(cfg.disk_path) / \"supervisely\" / \"2023*\"\n",
    "dataDirs = [Path(p) for p in glob.glob(str(supervisely_path))]\n",
    "\n",
    "for dataDir in dataDirs:\n",
    "    # initialize COCO api for instance annotations\n",
    "    coco = COCO(dataDir.joinpath(\"annotations/instances.json\"))\n",
    "    cats = coco.loadCats(coco.getCatIds())\n",
    "    nms = [cat[\"name\"] for cat in cats]\n",
    "    catIds = coco.getCatIds(catNms=nms)\n",
    "    imgIds = coco.getImgIds()\n",
    "\n",
    "    # fig = plt.figure(figsize=(20, 10), dpi=100)\n",
    "\n",
    "    df_pred_shapes = dict(\n",
    "        category=[],\n",
    "        confidence=[],\n",
    "        tile_path=[],\n",
    "        project_name=[],\n",
    "        tile_fname=[],\n",
    "        geometry=[],\n",
    "    )\n",
    "    df_seg_shapes = dict(\n",
    "        category=[], tile_path=[], tile_fname=[], project_name=[], geometry=[]\n",
    "    )\n",
    "\n",
    "    projName = (\n",
    "        Path(glob.glob(f\"{str(dataDir)}/images/*\")[0])\n",
    "        .stem.split(\"_\")[0]\n",
    "        .replace(\" \", \"_\")\n",
    "    )\n",
    "\n",
    "    for imgId in tqdm.tqdm(imgIds):\n",
    "        # COCO format image path and annotation\n",
    "        img = coco.loadImgs(imgId)[0]\n",
    "        imgPath = dataDir.joinpath(\"images\").joinpath(img[\"file_name\"])\n",
    "        anns = {}\n",
    "        for catId in catIds:\n",
    "            anns[catId] = coco.loadAnns(\n",
    "                coco.getAnnIds(imgIds=img[\"id\"], catIds=[catId], iscrowd=None)\n",
    "            )\n",
    "\n",
    "        # Load image\n",
    "        image_pil, image = utils.load_image(imgPath)\n",
    "\n",
    "        # Tags\n",
    "        if cfg.fixed_tags:\n",
    "            tags = \",\".join(cfg.fixed_tags)\n",
    "        else:\n",
    "            # # Find tags with RAM\n",
    "            # ram_model = ram_model.to(cfg.DEVICE)\n",
    "            # raw_image = image_pil.resize((384, 384))\n",
    "            # raw_image = transform(raw_image).unsqueeze(0).to(cfg.DEVICE)\n",
    "            # res = inference_ram(raw_image, ram_model)\n",
    "            # tags = res[0].replace(\" |\", \",\")\n",
    "            raise NotImplementedError(\"RAM not implemented\")\n",
    "\n",
    "        # Find bounding boxes with grounding dino\n",
    "        boxes_filt, scores, pred_phrases = utils.get_grounding_output(\n",
    "            dino_model,\n",
    "            image,\n",
    "            tags,\n",
    "            cfg.DINO_BOX_THRESHOLD,\n",
    "            cfg.DINO_TEXT_THRESHOLD,\n",
    "            device=cfg.DEVICE,\n",
    "        )\n",
    "\n",
    "        # Resize boxes\n",
    "        size = image_pil.size\n",
    "        H, W = size[1], size[0]\n",
    "        for i in range(boxes_filt.size(0)):\n",
    "            boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H]).to(cfg.device)\n",
    "            boxes_filt[i][:2] -= boxes_filt[i][2:] / 2\n",
    "            boxes_filt[i][2:] += boxes_filt[i][:2]\n",
    "\n",
    "        # use NMS to handle overlapped boxes\n",
    "        boxes_filt = boxes_filt.cpu()\n",
    "        nms_idx = (\n",
    "            torchvision.ops.nms(boxes_filt, scores, cfg.IOU_THRESHOLD).numpy().tolist()\n",
    "        )\n",
    "        if cfg.DO_IOU_MERGE:\n",
    "            boxes_filt_clean = boxes_filt[nms_idx]\n",
    "            pred_phrases_clean = [pred_phrases[idx] for idx in nms_idx]\n",
    "            # print(f\"NMS: before {boxes_filt.shape[0]} boxes, after {boxes_filt_clean.shape[0]} boxes\")\n",
    "        else:\n",
    "            boxes_filt_clean = boxes_filt\n",
    "            pred_phrases_clean = pred_phrases\n",
    "\n",
    "        # Segment objects with SAM\n",
    "        image_np = np.array(image_pil)\n",
    "        sam_predictor.set_image(image_np)\n",
    "        transformed_boxes = sam_predictor.transform.apply_boxes_torch(\n",
    "            boxes_filt_clean, image_np.shape[:2]\n",
    "        ).to(cfg.DEVICE)\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes.to(cfg.DEVICE),\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        # Find tile bounds (X, Y) based on name\n",
    "        imgName = \".\".join(coco.imgs[imgId][\"file_name\"].split(\".\")[:-1])\n",
    "        tile1a = imgName.split(\"_\")[0]\n",
    "        tile1b = int(imgName.split(\"_\")[-1])\n",
    "        cellfile = df_tilebounds[\n",
    "            (df_tilebounds.index == tile1b) & (df_tilebounds.name == tile1a)\n",
    "        ].copy()\n",
    "        assert len(cellfile) == 1\n",
    "        cellfile = cellfile.iloc[0, :].copy()\n",
    "        xstep = (cellfile.xmax - cellfile.xmin) / image_np.shape[1]\n",
    "        ystep = (cellfile.ymax - cellfile.ymin) / image_np.shape[0]\n",
    "\n",
    "        affine = [xstep, 0, cellfile.xmin, 0, -ystep, cellfile.ymax, 0, 0, 1]\n",
    "\n",
    "        # SAM masks\n",
    "        assert len(pred_phrases_clean) == len(masks)\n",
    "        shapes, titles = [], []\n",
    "        for cat_title, mask in zip(pred_phrases_clean, masks):\n",
    "            mask = mask.cpu().numpy()\n",
    "            cat_shapes = rasterio.features.shapes(\n",
    "                mask.astype(np.uint8), mask=mask, connectivity=4, transform=affine\n",
    "            )\n",
    "            for shape, _ in cat_shapes:\n",
    "                title, confidence = cat_title.replace(\")\", \"\").split(\"(\")\n",
    "                shape = shapely.geometry.shape(shape).simplify(\n",
    "                    0.01, preserve_topology=True\n",
    "                )\n",
    "                if shape.area > 0.01:\n",
    "                    df_pred_shapes[\"category\"].append(title)\n",
    "                    df_pred_shapes[\"confidence\"].append(confidence)\n",
    "                    df_pred_shapes[\"geometry\"].append(shape)\n",
    "                    df_pred_shapes[\"tile_path\"].append(str(imgPath))\n",
    "                    df_pred_shapes[\"tile_fname\"].append(imgPath.stem)\n",
    "                    df_pred_shapes[\"project_name\"].append(projName)\n",
    "\n",
    "        # COCO masks\n",
    "        for catId, catName in zip(catIds, nms):\n",
    "            for ann in anns[catId]:\n",
    "                t = coco.imgs[ann[\"image_id\"]]\n",
    "\n",
    "                if isinstance(ann[\"segmentation\"][\"counts\"], list):\n",
    "                    rle = pycocotools.mask.frPyObjects(\n",
    "                        [ann[\"segmentation\"]], t[\"height\"], t[\"width\"]\n",
    "                    )\n",
    "                else:\n",
    "                    rle = [ann[\"segmentation\"]]\n",
    "\n",
    "                m = pycocotools.mask.decode(rle)[:, :, 0]\n",
    "                cat_shapes = rasterio.features.shapes(\n",
    "                    m.astype(np.uint8), mask=m, connectivity=4, transform=affine\n",
    "                )\n",
    "\n",
    "                for shape, _ in cat_shapes:\n",
    "                    title, confidence = cat_title.replace(\")\", \"\").split(\"(\")\n",
    "                    shape = shapely.geometry.shape(shape).simplify(\n",
    "                        0.01, preserve_topology=True\n",
    "                    )\n",
    "                    if shape.area > 0.01:\n",
    "                        df_seg_shapes[\"category\"].append(catName)\n",
    "                        df_seg_shapes[\"geometry\"].append(shape)\n",
    "\n",
    "                        df_seg_shapes[\"tile_path\"].append(str(imgPath))\n",
    "                        df_seg_shapes[\"tile_fname\"].append(imgPath.stem)\n",
    "                        df_seg_shapes[\"project_name\"].append(projName)\n",
    "\n",
    "        # # Clear figure and add axis\n",
    "        # fig.clear()\n",
    "        # axs = fig.subplots(1, 2, squeeze=False)\n",
    "\n",
    "        # # Draw supervisely image (COCO format)\n",
    "        # axs[0,0].imshow(image_np)\n",
    "        # i_end = 0\n",
    "        # legend_handles = []\n",
    "        # for catId, catName in zip(catIds, nms):\n",
    "        #     plt.sca(axs[0,0])\n",
    "        #     coco.showAnns(anns[catId], draw_bbox=True)\n",
    "        #     for ann in anns[catId]:\n",
    "        #         plt.text(ann[\"bbox\"][0], ann[\"bbox\"][1] - 5, catName, fontsize=10)\n",
    "\n",
    "        # # draw RAM-Dino-SAM image\n",
    "        # axs[0,1].imshow(image_np)\n",
    "        # for mask in masks:\n",
    "        #     show_mask(mask.cpu().numpy(), axs[0,1], random_color=True)\n",
    "\n",
    "        # for box, label in zip(boxes_filt_clean, pred_phrases_clean):\n",
    "        #     show_box(box.numpy(), axs[0,1], label)\n",
    "\n",
    "        # # Set titles\n",
    "        # axs[0,0].set_title(\"Supervisely\")\n",
    "        # axs[0,1].set_title(f\"RAM-DINO-SAM, tags: {tags}\", wrap=True)\n",
    "\n",
    "        # # Dont show axis\n",
    "        # axs[0,0].axis(\"off\")\n",
    "        # axs[0,1].axis(\"off\")\n",
    "\n",
    "        # # Save figure\n",
    "        # if fixed_tags:\n",
    "        #     if USE_SAM_HQ:\n",
    "        #         plt_dir = pathlib.Path(\"test_output/fixed_tags_hq\")\n",
    "        #     else:\n",
    "        #         plt_dir = pathlib.Path(\"test_output/fixed_tags\")\n",
    "        # else:\n",
    "        #     if USE_SAM_HQ:\n",
    "        #         plt_dir = pathlib.Path(\"test_output/ram_tags_hq\")\n",
    "        #     else:\n",
    "        #         plt_dir = pathlib.Path(\"test_output/ram_tags\")\n",
    "        # plt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # fig.savefig(plt_dir.joinpath(imgPath.name), bbox_inches=\"tight\")\n",
    "\n",
    "    df_pred_shapes = gpd.GeoDataFrame(df_pred_shapes, crs=\"epsg:28992\")\n",
    "    if cfg.USE_SAM_HQ:\n",
    "        df_pred_shapes.to_file(out_dir / f\"/fix_tags_hq_{projName}.gpkg\")\n",
    "    else:\n",
    "        df_pred_shapes.to_file(out_dir / f\"fix_tags_{projName}.gpkg\")\n",
    "\n",
    "    df_seg_shapes = gpd.GeoDataFrame(df_seg_shapes, crs=\"epsg:28992\")\n",
    "    df_seg_shapes.to_file(out_dir / f\"supervisely_tags_{projName}.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seg_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
