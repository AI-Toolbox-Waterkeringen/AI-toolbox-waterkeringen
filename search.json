[
  {
    "objectID": "more.html",
    "href": "more.html",
    "title": "Meer over de AI Toolbox",
    "section": "",
    "text": "De AI Toolbox is ontwikkeld voor data scientists bij waterschappen. Heeft u algemene vragen over de toolbox die niet technisch van aard zijn, neem dan contact met ons op.\nOp deze pagina zijn voorbeelden te vinden van scripts in de AI Toolbox Waterkeringen. Klik links op een voorbeeld. Daarmee krijgt een een overzicht van wat een algoritme kan en hoe het te gebruiken is. De voorbeelden zijn gemaakt in Jupyter Notebooks en zijn ook reproduceerbaar.\nHet linkermenu bevat alle links naar de voorbeelden.",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox"
    ]
  },
  {
    "objectID": "examples/scheurdetectie_voorbeeld.html",
    "href": "examples/scheurdetectie_voorbeeld.html",
    "title": "Scheurdetectie op droogtegevoelige dijken",
    "section": "",
    "text": "Dit is een voorbeeldnotebook om met een Unet model droogtescheuren op droogtegevoelige dijken te detecteren. Het model is getraind op een verschillende dijken in het beheergebied van HHNK en Wetterskip Fryslan.\n\nStel de ‘working directory’ in\nDit instellen is zodat de relatieve paden tot de scripts en het model goed worden opgepakt door dit notebook.\n\n\nCode\n%cd \"../../scripts/scheur-detectie/\"\n\n\n\n\nLaad de benodigde imports voor dit notebook\n\n\nCode\n# Imports\n\nimport tensorflow as tf\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom rasterio.windows import Window\nfrom rasterio.plot import show\nfrom tensorflow.keras.models import load_model\n\n\n\n\nLaad het getrainde model in de notebook\nInclusief de tijdens trainen gebruikte ‘custom functions’ zodat het model geen foutmelding geeft tijdens het voorspellen.\n\n\nCode\n# Model V2 - 2023 with Wetterskip\nfolds = {4: {\"EPOCHS\": 150, \"batch\": 80, \"LR\": 0.0001}}\nfold = 4\nweights_folder = (\n    \"saved_weights/model_v2_plateau/\"\n    + \"BATCHSIZE_{}_LR_{}_EPOCH_{}_Fold_{}\".format(\n        folds[fold][\"batch\"], folds[fold][\"LR\"], folds[fold][\"EPOCHS\"], fold\n    )\n)\n\n\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    smooth = 1e-15\n    return (2.0 * intersection + smooth) / (\n        tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth\n    )\n\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)\n\n\nmodel = load_model(\n    weights_folder, custom_objects={\"dice_loss\": dice_loss}, compile=False\n)\n\n\n\n\nHieronder een voorbeeld stuk dijk met droogtescheuren\n\n\nCode\ninputfolder = \"voorbeeld/\"\ninputtif = \"testfile_cracks_1024x2048.tif\"\n\nfig, ax = plt.subplots(1, figsize=(16, 32))\nraster_rio = rio.open(inputfolder + inputtif)\nshow(raster_rio, ax=ax)\n\n\n\n\nHaal het voorbeeldraster in opgesneden ‘windows’ door het model heen\nHier wordt door het raster bestand heen ‘geloopt’ met ‘windows’ van 256x256 pixels en elke ‘window’ zal een bijbehorende voorspelling krijgen. Deze voorspellingen zullen naar een nieuw gemaakt raster worden geschreven zodat het eindresultaat een raster met dezelfde afmetingen als het origineel is waarin aangegeven is waar geen scheur (0) en wel een scheur (1) zit.\n\n\nCode\n# Laad de folders\ninputfolder = \"voorbeeld/\"\ninputtif = \"testfile_cracks_1024x2048\"\nextension = \".tif\"\nlabelfolder = \"voorbeeld/label/\"\n\nfile_location = inputfolder + inputtif + extension\nlabel_location = labelfolder + inputtif + \"_label\" + extension\n\n# Update het originele profiel naar 1 band\nraster_file = rio.open(file_location)\nprofile = raster_file.profile.copy()\nprofile.update(count=1)\n\n# Maak het bestaand aan\nwith rio.open(str(label_location), \"w\", **profile) as out_file:\n    out_file.write(np.zeros([1, 1, 1]), window=Window.from_slices((0, 1), (0, 1)))\n\n# Totaal aantal windows\ntotal = ((profile[\"height\"] - 256) / 256) * ((profile[\"width\"] - 256) / 256)\n\n# Loop over de windows en voorspel\nfor idx in range(0, profile[\"height\"] - 256, 256):\n    for idy in range(0, profile[\"width\"] - 256, 256):\n        print(idx, \" of \", total)\n        wdw = Window.from_slices((idx, idx + 256), (idy, idy + 256))\n\n        with rio.open(str(file_location), \"r\") as out_file:\n            x = out_file.read(window=wdw)\n\n        x = x[:3, :, :]\n        x = np.rollaxis(x, 0, 3)\n\n        threshold = 0.8\n\n        y_pred = model.predict(np.expand_dims(x / 255, axis=0))[0]\n        y_pred_threshold = np.where((y_pred * 1000) &gt; (threshold), 1, 0)\n\n        with rio.open(str(label_location), \"r+\", **profile) as out_file:\n            out_file.write(y_pred_threshold.reshape(1, 256, 256), window=wdw)\n\n\n\n\nPlot het eindresultaat\nDoor middel van het laden van het voorspel raster als een ‘mask’ kan het over het originele stuk dijk heen worden gevisualiseerd waardoor de scheuren worden aangegeven op de dijk.\n\n\nCode\n# Maak een figuur aan\nfig, ax = plt.subplots(1, figsize=(16, 32))\n\n# Visualiseer het origineel\ninputfolder = \"voorbeeld/\"\ninputtif = \"testfile_cracks_1024x2048.tif\"\n\nraster_rio = rio.open(inputfolder + inputtif)\nraster_array = raster_rio.read()\n\nshow(raster_array, ax=ax)\n\n# Visualiseer hieroveer heen het label\ninputfolder = \"voorbeeld/label/\"\ninputtif = \"testfile_cracks_1024x2048_label.tif\"\n\nraster_rio = rio.open(inputfolder + inputtif)\nlabel_array = raster_rio.read()\n\nmasked_label = np.ma.masked_where(label_array == 0, label_array)\n\nax.imshow(masked_label.reshape(1024, 2048, 1), cmap=\"viridis\")",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Scheurdetectie op droogtegevoelige dijken"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Over ons",
    "section": "",
    "text": "Dit is een initiatief van HHNK, het PIW-programma van STOWA en RWS-WVL en HKV. We willen hiermee beheer en onderhoud van waterkeringen efficiënter en effectiever maken.\nWaterschappen inspecteren jaarlijks vele duizenden kilometers waterkeringen. Om inzicht te krijgen in actuele sterkte van waterkeringen wordt het steeds belangrijker om te kijken naar verschillen in conditie ten opzichte van de uitgangspunten van de toetsing en beoordeling. Dit soort inspecties zijn arbeidsintensief. Beheerders geven aan dat het handmatig vastleggen van schadebeelden en het verwerken van inspecties zeer veel tijd kost. Onze algoritmes bieden daarvoor een oplossing door consequent foto’s te vertalen naar informatie over degradatie, schadebeelden en veranderingen.\nDe AI toolbox is een open ontwikkeling. Heb je vragen of wil je meewerken aan nieuwe ontwikkelingen, neem dan contact met ons op."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Trainingsdata",
    "section": "",
    "text": "In de trainingsdata staat alle data die ten grondslag ligt aan de ontwikkelde algoritmes. Om algoritmes door te ontwikkelen vinden wij het belangrijk dat je gebruik kunt maken van de originele gemarkeerde en gelabelde data. Het is mogelijk eigen databronnen toe te voegen. Omdat we in de opstartfase zitten is dit niet geautomatiseerd. Wil je deze data gebruiken of data toevoegen? Neem contact met ons op.\nIn onderstaande tabel zijn de eerste twee niveaus van de mappen in de opslag te zien.\n\n\n\nNaam\n\n\n\n\nopschot-detectie/\n\n\n    opschot-detectie/supervisely/\n\n\nscheurdetectie/\n\n\n    scheurdetectie/ilpendam/\n\n\n    scheurdetectie/jisp/"
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html",
    "href": "examples/opschotdetectie_voorbeeld.html",
    "title": "Opschotdetectie",
    "section": "",
    "text": "Deze notebook bevat een voorbeeld van opschotdetectie. Opschot zien we als ongewenste, houtachtige begroeiing op een steenbekleding van een waterkering. De methode om opschot te detecteren is in dit voorbeeld gebaseerd op zero-shot inference met de foundation models GroundingDino en SegmentAnything.\nCode\nimport os\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.axes\nfrom typing import List, Tuple, Iterable\nfrom PIL import Image\nimport torchvision.transforms as T\n\nimport groundingdino.util.inference\nimport groundingdino.util.utils\n\nfrom hydra import initialize, compose\n\nwith initialize(\"config\", version_base=None):\n    cfg = compose(\"config.yaml\")",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#voorbeeld-foto-met-opschot",
    "href": "examples/opschotdetectie_voorbeeld.html#voorbeeld-foto-met-opschot",
    "title": "Opschotdetectie",
    "section": "Voorbeeld foto met opschot",
    "text": "Voorbeeld foto met opschot\nWe passen de methode toe op een drone foto van een kering met steenbekleding. Op de steenbekleding is duidelijk opschot aanwezig:\n\n\nCode\nimgPath = \"images/test.jpeg\"\n\n# Load image\nimage_pil = Image.open(imgPath).convert(\"RGB\")\n\n\n# Define transformations\ntransform = T.Compose(\n    [\n        T.Resize(800),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n# Apply transformations\nimage = transform(image_pil)\nimage_np = np.array(image_pil)\n\nfig, ax = plt.subplots(figsize=(5, 5), dpi=100)\nax.imshow(image_pil)\nax.axis(\"off\")\nfig.tight_layout();",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#foundation-models",
    "href": "examples/opschotdetectie_voorbeeld.html#foundation-models",
    "title": "Opschotdetectie",
    "section": "Foundation models",
    "text": "Foundation models\nOm opschot te detecteren, moeten we de locatie van opschot bepalen en vervolgens uitknippen. De locatie bepalen we met behulp van GroundingDino, waarna we de precieze locatie uitknippen met SegmentAnything. Om dit te doen moeten we eerst de twee modellen inladen.\n\n\nCode\n# Load Grounding Dino Model\ndino_model = groundingdino.util.inference.load_model(\n    cfg.GROUNDING_DINO_CONFIG_PATH,\n    cfg.GROUNDING_DINO_CHECKPOINT_PATH,\n    device=cfg.DEVICE,\n)\n\n# Load Segment Anything Model (SAM)\nif cfg.USE_SAM_HQ:\n    from segment_anything_hq import SamPredictor as SamPredictor_hq\n    from segment_anything_hq import sam_model_registry as sam_model_registry_hq\n\n    sam = sam_model_registry_hq[cfg.SAM_HQ_ENCODER_VERSION](\n        checkpoint=cfg.SAM_HQ_CHECKPOINT_PATH\n    ).to(device=cfg.DEVICE)\n    sam_predictor = SamPredictor_hq(sam)\nelse:\n    from segment_anything import SamPredictor, sam_model_registry\n\n    sam = sam_model_registry[cfg.SAM_ENCODER_VERSION](\n        checkpoint=cfg.SAM_CHECKPOINT_PATH\n    ).to(device=cfg.DEVICE)\n    sam_predictor = SamPredictor(sam)",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#trefwoorden-voor-groundingdino-om-opschot-te-detecteren",
    "href": "examples/opschotdetectie_voorbeeld.html#trefwoorden-voor-groundingdino-om-opschot-te-detecteren",
    "title": "Opschotdetectie",
    "section": "Trefwoorden voor GroundingDino om opschot te detecteren",
    "text": "Trefwoorden voor GroundingDino om opschot te detecteren\nGroundingDino detecteert objecten op basis van een of meerdere trefwoorden (‘tags’). In dit voorbeeld geven we de trefwoorden ‘bush’, ‘shrub’, ‘tree’ en ‘plant’ mee aan GroundingDino om opschot te detecteren. GroundingDino produceert op basis van deze trefwoorden bounding boxes in de figuur waar deze trefwoorden worden aangetroffen. Op basis van Non-maximum-Supression worden overlappende bounding boxes tot 1 box samengevoegd.\n\n\nCode\ntag_list = [\"bush\", \"shrub\", \"tree\", \"plant\"]\ntags = \". \".join(tag_list)\n\n\ndef get_grounding_output(\n    model: torch.nn.Module,\n    image: torch.Tensor,\n    caption: str,\n    box_threshold: float,\n    text_threshold: float,\n    device: str = \"cpu\",\n) -&gt; Tuple[torch.Tensor, torch.Tensor, List[str]]:\n    \"\"\"\n    Process an image and caption through a model to generate grounded outputs,\n    including filtered bounding boxes and corresponding text phrases.\n\n    Parameters:\n    - model (torch.nn.Module): The model to process the input data.\n    - image (torch.Tensor): The image tensor.\n    - caption (str): The caption string related to the image.\n    - box_threshold (float): The threshold value to filter the bounding boxes based on confidence scores.\n    - text_threshold (float): The threshold value to filter the text based on logits.\n    - device (str, optional): The device type, 'cpu' or 'cuda', where the computation will take place. Defaults to 'cpu'.\n\n    Returns:\n    - tuple:\n        - filtered_boxes (torch.Tensor): The filtered bounding boxes.\n        - scores (torch.Tensor): The confidence scores of the phrases.\n        - pred_phrases (list of str): The predicted phrases associated with the bounding boxes.\n    \"\"\"\n    # Prepare caption\n    caption = caption.lower().strip()\n    if not caption.endswith(\".\"):\n        caption += \".\"\n\n    # Move model and image to the specified device\n    model = model.to(device)\n    image = image.to(device)\n\n    # Generate predictions\n    try:\n        with torch.no_grad():\n            outputs = model(\n                image.unsqueeze(0), captions=[caption]\n            )  # Ensure image is 4D\n        logits = outputs[\"pred_logits\"].sigmoid()[0]  # (num_queries, num_classes)\n        boxes = outputs[\"pred_boxes\"][0]  # (num_queries, 4)\n\n        # Filter outputs based on thresholds\n        max_logits = logits.max(dim=1)[0]\n        filt_mask = max_logits &gt; box_threshold\n        logits_filt = logits[filt_mask]\n        boxes_filt = boxes[filt_mask]\n\n        # Prepare phrases and scores\n        tokenizer = model.tokenizer\n        tokenized = tokenizer(caption)\n        pred_phrases, scores = [], []\n        for logit, box in zip(logits_filt, boxes_filt):\n            pred_phrase = groundingdino.util.utils.get_phrases_from_posmap(\n                logit &gt; text_threshold, tokenized, tokenizer\n            )\n            pred_phrases.append(f\"{pred_phrase} ({logit.max().item():.4f})\")\n            scores.append(logit.max().item())\n\n        return boxes_filt, torch.tensor(scores), pred_phrases\n    except Exception as e:\n        raise Exception(f\"An error occurred during model prediction: {e}\")\n\n\n# Find bounding boxes with grounding dino\nboxes_filt, scores, pred_phrases = get_grounding_output(\n    dino_model,\n    image,\n    tags,\n    0.35,\n    0.25,\n    device=cfg.DEVICE,\n)\nboxes_filt = boxes_filt.cpu()\n\n# Resize boxes\nsize = image_pil.size\nH, W = size[1], size[0]\nfor i in range(boxes_filt.size(0)):\n    boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])\n    boxes_filt[i][:2] -= boxes_filt[i][2:] / 2\n    boxes_filt[i][2:] += boxes_filt[i][:2]\n\n# use NMS to handle overlapped boxes\nnms_idx = torchvision.ops.nms(boxes_filt, scores, 0.5).numpy().tolist()\nif cfg.DO_IOU_MERGE:\n    boxes_filt_clean = boxes_filt[nms_idx]\n    pred_phrases_clean = [pred_phrases[idx] for idx in nms_idx]\n    print(\n        f\"NMS: before {boxes_filt.shape[0]} boxes, after {boxes_filt_clean.shape[0]} boxes\"\n    )\nelse:\n    boxes_filt_clean = boxes_filt\n    pred_phrases_clean = pred_phrases\n\n\ndef show_box(box: Iterable[float], ax: matplotlib.axes.Axes, label: str) -&gt; None:\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - x0, box[3] - y0\n    rect = plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=\"none\", lw=2)\n    ax.add_patch(rect)\n    ax.text(\n        x0,\n        y0,\n        label,\n        verticalalignment=\"top\",\n        color=\"white\",\n        fontsize=8,\n        bbox={\"facecolor\": \"black\", \"alpha\": 0.5},\n    )\n    return None\n\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n\nax = axs[0, 0]\nax.imshow(image_np)\nax.set_title(\"Origineel\", wrap=True)\nax.axis(\"off\")\n\nax = axs[0, 1]\nax.imshow(image_np)\nfor box, label in zip(boxes_filt_clean, pred_phrases_clean):\n    show_box(box.numpy(), ax, label)\nax.set_title(f\"GroundingDino tags: {tag_list}\", wrap=True)\nax.axis(\"off\")\nfig.tight_layout();",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#uitknippen-van-opschot-met-segmentanything",
    "href": "examples/opschotdetectie_voorbeeld.html#uitknippen-van-opschot-met-segmentanything",
    "title": "Opschotdetectie",
    "section": "Uitknippen van opschot met SegmentAnything",
    "text": "Uitknippen van opschot met SegmentAnything\nOp basis van de zojuist gedetecteerde objecten, kunnen met SegmentAnything de contouren gedetecteerd worden.\n\n\nCode\ndef show_mask(\n    mask: np.ndarray, ax: matplotlib.axes.Axes, random_color: bool = False\n) -&gt; None:\n    if random_color:\n        color = np.random.rand(3)  # Generates three random floats between 0 and 1\n        color = np.append(color, 0.6)  # Add alpha for transparency\n    else:\n        color = np.array(\n            [30 / 255, 144 / 255, 255 / 255, 0.6]\n        )  # Deep sky blue with transparency\n\n    h, w = mask.shape\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    return None\n\n\n# Segment objects with SAM\nsam_predictor.set_image(image_np)\ntransformed_boxes = sam_predictor.transform.apply_boxes_torch(\n    boxes_filt_clean, image_np.shape[:2]\n).to(cfg.DEVICE)\nmasks, _, _ = sam_predictor.predict_torch(\n    point_coords=None,\n    point_labels=None,\n    boxes=transformed_boxes.to(cfg.DEVICE),\n    multimask_output=False,\n)\n\nfor cat_title, mask in zip(pred_phrases_clean, masks):\n    mask = mask.cpu().numpy()\n\n# Setup figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n\nax = axs[0, 0]\nax.imshow(image_np)\nax.set_title(\"Origineel\", wrap=True)\nax.axis(\"off\")\n\nax = axs[0, 1]\nax.imshow(image_np)\nfor mask in masks:\n    show_mask(mask[0, ...].cpu().numpy(), ax, random_color=False)\nfor box, label in zip(boxes_filt_clean, pred_phrases_clean):\n    show_box(box.numpy(), ax, label)\nax.set_title(f\"GroundingDino + SegmentAnything, tags: {tag_list}\", wrap=True)\nax.axis(\"off\")\nfig.tight_layout();",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Dit is een open ontwikkeling. Alle algoritmes zijn open source beschikbaar en we streven ernaar om de data science community in de watersector hiermee te enthousiasmeren, te stimuleren en te motiveren om samen met ons nieuwe algoritmes te ontwikkelen.\nWil je meewerken aan nieuwe ontwikkelingen, wil je toegang krijgen tot de trainingsdata (of wil je trainingsdata toevoegen), of heb je algemene vragen: neem dan contact met ons op via aitoolboxwaterkeringen@hkv.nl.\nVoor meer informatie neem contact op met:\n\nGuy Dupuits – inhoudelijk expert op waterkeringen en AI algoritmes (dupuits@hkv.nl)\nErik Vastenburg – waterveiligheidsadviseur bij HHNK (e.vastenburg@hhnk.nl)"
  }
]