[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Dit is een open ontwikkeling. Alle algoritmes zijn open source beschikbaar en we streven ernaar om de data science community in de watersector hiermee te enthousiasmeren, te stimuleren en te motiveren om samen met ons nieuwe algoritmes te ontwikkelen.\nWil je meewerken aan nieuwe ontwikkelingen, wil je toegang krijgen tot de trainingsdata (of wil je trainingsdata toevoegen), of heb je algemene vragen: neem dan contact met ons op via aitoolboxwaterkeringen@hkv.nl.\nVoor meer informatie neem contact op met: - Guy Dupuits – inhoudelijk expert op waterkeringen en AI algoritmes (dupuits@hkv.nl) - Erik Vastenburg – waterveiligheidsadviseur bij HHNK (e.vastenburg@hhnk.nl)"
  },
  {
    "objectID": "digigids.html",
    "href": "digigids.html",
    "title": "DigiGids 2.0",
    "section": "",
    "text": "In de Digigids 2.0 staat alle trainingsdata die ten grondslag ligt aan de ontwikkelde algoritmes. Om algoritmes door te ontwikkelen vinden wij het belangrijk dat je gebruik kunt maken van de originele gemarkeerde en gelabelde data. Het is mogelijk eigen databronnen toe te voegen. Omdat we in de opstartfase zitten is dit niet geautomatiseerd. Wil je deze data gebruiken of data toevoegen? Neem contact met ons op.\nIn onderstaande tabel zijn de eerste twee niveaus van de mappen in de opslag.\n\n\n\nNaam\n\n\n\n\nopschot-detectie/\n\n\n    opschot-detectie/supervisely/\n\n\nscheurdetectie/\n\n\n    scheurdetectie/ilpendam/\n\n\n    scheurdetectie/jisp/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Over ons",
    "section": "",
    "text": "Dit is een initiatief van HHNK, STOWA en HKV. We denken hiermee beheer en onderhoud van waterkeringen efficiënter en effectiever te maken.\nWaterschappen inspecteren jaarlijks vele duizenden kilometers waterkeringen. Om inzicht te krijgen in actuele sterkte van waterkeringen wordt het steeds belangrijker om te kijken naar verschillen in conditie ten opzichte van de uitgangspunten van de toetsing en beoordeling. Dit soort inspecties zijn arbeidsintensief. Beheerders geven aan dat het handmatig vastleggen van schadebeelden en het verwerken van inspecties zeer veel tijd kost. Onze algoritmes bieden daarvoor een oplossing door consequent foto’s te vertalen naar informatie over degradatie, schadebeelden en veranderingen.\nDe AI toolbox is een open ontwikkeling. Heb je vragen of wil je meewerken aan nieuwe ontwikkelingen, neem dan contact met ons op."
  },
  {
    "objectID": "more.html",
    "href": "more.html",
    "title": "Meer over de AI Toolbox",
    "section": "",
    "text": "De AI Toolbox is ontwikkeld voor data scientists bij waterschappen. Heeft u algemene vragen over de toolbox die niet technisch van aard zijn, neem dan contact met ons op.\nOp deze pagina zijn voorbeelden te vinden van scripts in de AI Toolbox Waterkeringen. Klik links op een voorbeeld. Daarmee krijgt een een overzicht van wat een algoritme kan en hoe het te gebruiken is. De voorbeelden zijn gemaakt in Jupyter Notebooks en zijn ook reproduceerbaar.\nHet linkermenu bevat alle links naar de voorbeelden.",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html",
    "href": "examples/opschotdetectie_voorbeeld.html",
    "title": "Opschotdetectie",
    "section": "",
    "text": "Deze notebook bevat een voorbeeld van opschotdetectie. Opschot zien we als ongewenste, houtachtige begroeiing op een steenbekleding van een waterkering. De methode om opschot te detecteren is in dit voorbeeld gebaseerd op zero-shot inference met de foundation models GroundingDino en SegmentAnything.\nCode\nimport os\nimport torch\nimport hydra\nimport torchvision\nimport numpy as np\nimport geopandas as gpd\nfrom pathlib import Path\nimport tqdm\nimport rasterio.features\nimport shapely.geometry\nimport matplotlib.pyplot as plt\nimport matplotlib.axes\nfrom typing import Union, List, Tuple, Iterable, Dict\nimport pycocotools.mask\nfrom omegaconf import DictConfig\nfrom PIL import Image\nimport torchvision.transforms as T\nimport logging\nfrom logging.config import fileConfig\n\nimport groundingdino.util.inference\nimport groundingdino.util.utils\n\nfrom hydra import initialize, compose\n\nwith initialize(\"config\", version_base=None):\n    cfg = compose(\"config.yaml\")",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#voorbeeld-foto-met-opschot",
    "href": "examples/opschotdetectie_voorbeeld.html#voorbeeld-foto-met-opschot",
    "title": "Opschotdetectie",
    "section": "Voorbeeld foto met opschot",
    "text": "Voorbeeld foto met opschot\nWe passen de methode toe op een drone foto van een kering met steenbekleding. Op de steenbekleding is duidelijk opschot aanwezig:\n\n\nCode\nimgPath = 'images/test.jpeg'\n\n# Load image\nimage_pil = Image.open(imgPath).convert(\"RGB\")\n\n\n# Define transformations\ntransform = T.Compose(\n    [\n        T.Resize(800),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n# Apply transformations\nimage = transform(image_pil)\nimage_np = np.array(image_pil)\n\nfig, ax = plt.subplots(figsize=(5, 5), dpi=100)\nax.imshow(image_pil)\nax.axis(\"off\")\nfig.tight_layout();",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#foundation-models",
    "href": "examples/opschotdetectie_voorbeeld.html#foundation-models",
    "title": "Opschotdetectie",
    "section": "Foundation models",
    "text": "Foundation models\nOm opschot te detecteren, moeten we de locatie van opschot bepalen en vervolgens uitknippen. De locatie bepalen we met behulp van GroundingDino, waarna we de precieze locatie uitknippen met SegmentAnything. Om dit te doen moeten we eerst de twee modellen inladen.\n\n\nCode\n# Load Grounding Dino Model\ndino_model = groundingdino.util.inference.load_model(\n    cfg.GROUNDING_DINO_CONFIG_PATH,\n    cfg.GROUNDING_DINO_CHECKPOINT_PATH,\n    device=\"cuda:1\",\n)\n\n# Load Segment Anything Model (SAM)\nif cfg.USE_SAM_HQ:\n    from segment_anything_hq import SamPredictor as SamPredictor_hq\n    from segment_anything_hq import sam_model_registry as sam_model_registry_hq\n\n    sam = sam_model_registry_hq[cfg.SAM_HQ_ENCODER_VERSION](\n        checkpoint=cfg.SAM_HQ_CHECKPOINT_PATH\n    ).to(device=\"cuda:1\")\n    sam_predictor = SamPredictor_hq(sam)\nelse:\n    from segment_anything import SamPredictor, sam_model_registry\n\n    sam = sam_model_registry[cfg.SAM_ENCODER_VERSION](\n        checkpoint=cfg.SAM_CHECKPOINT_PATH\n    ).to(device=\"cuda:1\")\n    sam_predictor = SamPredictor(sam)\n\n\n/opt/tljh/user/envs/opschotdetectie2/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025842427/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\nfinal text_encoder_type: bert-base-uncased",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#trefwoorden-voor-groundingdino-om-opschot-te-detecteren",
    "href": "examples/opschotdetectie_voorbeeld.html#trefwoorden-voor-groundingdino-om-opschot-te-detecteren",
    "title": "Opschotdetectie",
    "section": "Trefwoorden voor GroundingDino om opschot te detecteren",
    "text": "Trefwoorden voor GroundingDino om opschot te detecteren\nGroundingDino detecteert objecten op basis van een of meerdere trefwoorden (‘tags’). In dit voorbeeld geven we de trefwoorden ‘bush’, ‘shrub’, ‘tree’ en ‘plant’ mee aan GroundingDino om opschot te detecteren. GroundingDino produceert op basis van deze trefwoorden bounding boxes in de figuur waar deze trefwoorden worden aangetroffen. Op basis van Non-maximum-Supression worden overlappende bounding boxes tot 1 box samengevoegd.\n\n\nCode\ntag_list = [\"bush\", \"shrub\", \"tree\", \"plant\"]\ntags = \". \".join(tag_list)\n\ndef get_grounding_output(\n    model: torch.nn.Module,\n    image: torch.Tensor,\n    caption: str,\n    box_threshold: float,\n    text_threshold: float,\n    device: str = \"cpu\",\n) -&gt; Tuple[torch.Tensor, torch.Tensor, List[str]]:\n    \"\"\"\n    Process an image and caption through a model to generate grounded outputs,\n    including filtered bounding boxes and corresponding text phrases.\n\n    Parameters:\n    - model (torch.nn.Module): The model to process the input data.\n    - image (torch.Tensor): The image tensor.\n    - caption (str): The caption string related to the image.\n    - box_threshold (float): The threshold value to filter the bounding boxes based on confidence scores.\n    - text_threshold (float): The threshold value to filter the text based on logits.\n    - device (str, optional): The device type, 'cpu' or 'cuda', where the computation will take place. Defaults to 'cpu'.\n\n    Returns:\n    - tuple:\n        - filtered_boxes (torch.Tensor): The filtered bounding boxes.\n        - scores (torch.Tensor): The confidence scores of the phrases.\n        - pred_phrases (list of str): The predicted phrases associated with the bounding boxes.\n    \"\"\"\n    # Prepare caption\n    caption = caption.lower().strip()\n    if not caption.endswith(\".\"):\n        caption += \".\"\n\n    # Move model and image to the specified device\n    model = model.to(device)\n    image = image.to(device)\n\n    # Generate predictions\n    try:\n        with torch.no_grad():\n            outputs = model(\n                image.unsqueeze(0), captions=[caption]\n            )  # Ensure image is 4D\n        logits = outputs[\"pred_logits\"].sigmoid()[0]  # (num_queries, num_classes)\n        boxes = outputs[\"pred_boxes\"][0]  # (num_queries, 4)\n\n        # Filter outputs based on thresholds\n        max_logits = logits.max(dim=1)[0]\n        filt_mask = max_logits &gt; box_threshold\n        logits_filt = logits[filt_mask]\n        boxes_filt = boxes[filt_mask]\n\n        # Prepare phrases and scores\n        tokenizer = model.tokenizer\n        tokenized = tokenizer(caption)\n        pred_phrases, scores = [], []\n        for logit, box in zip(logits_filt, boxes_filt):\n            pred_phrase = groundingdino.util.utils.get_phrases_from_posmap(\n                logit &gt; text_threshold, tokenized, tokenizer\n            )\n            pred_phrases.append(f\"{pred_phrase} ({logit.max().item():.4f})\")\n            scores.append(logit.max().item())\n\n        return boxes_filt, torch.tensor(scores), pred_phrases\n    except Exception as e:\n        raise Exception(f\"An error occurred during model prediction: {e}\")\n\n# Find bounding boxes with grounding dino\nboxes_filt, scores, pred_phrases = get_grounding_output(\n    dino_model,\n    image,\n    tags,\n    0.35,\n    0.25,\n    device=\"cuda:1\",\n)\nboxes_filt =boxes_filt.cpu()\n\n # Resize boxes\nsize = image_pil.size\nH, W = size[1], size[0]\nfor i in range(boxes_filt.size(0)):\n    boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])\n    boxes_filt[i][:2] -= boxes_filt[i][2:] / 2\n    boxes_filt[i][2:] += boxes_filt[i][:2]\n\n# use NMS to handle overlapped boxes\nnms_idx = (\n    torchvision.ops.nms(boxes_filt, scores, 0.5).numpy().tolist()\n)\nif cfg.DO_IOU_MERGE:\n    boxes_filt_clean = boxes_filt[nms_idx]\n    pred_phrases_clean = [pred_phrases[idx] for idx in nms_idx]\n    print(f\"NMS: before {boxes_filt.shape[0]} boxes, after {boxes_filt_clean.shape[0]} boxes\")\nelse:\n    boxes_filt_clean = boxes_filt\n    pred_phrases_clean = pred_phrases\n\ndef show_box(box: Iterable[float], ax: matplotlib.axes.Axes, label: str) -&gt; None:\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - x0, box[3] - y0\n    rect = plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=\"none\", lw=2)\n    ax.add_patch(rect)\n    ax.text(\n        x0,\n        y0,\n        label,\n        verticalalignment=\"top\",\n        color=\"white\",\n        fontsize=8,\n        bbox={\"facecolor\": \"black\", \"alpha\": 0.5},\n    )\n    return None\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n\nax = axs[0, 0]\nax.imshow(image_np)\nax.set_title(\"Origineel\", wrap=True)\nax.axis(\"off\");\n\nax = axs[0, 1]\nax.imshow(image_np)\nfor box, label in zip(boxes_filt_clean, pred_phrases_clean):\n    show_box(box.numpy(), ax, label)\nax.set_title(f\"GroundingDino tags: {tag_list}\", wrap=True)\nax.axis(\"off\")\nfig.tight_layout();\n\n\n/opt/tljh/user/envs/opschotdetectie2/lib/python3.11/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/tljh/user/envs/opschotdetectie2/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/tljh/user/envs/opschotdetectie2/lib/python3.11/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n\n\nNMS: before 3 boxes, after 2 boxes",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  },
  {
    "objectID": "examples/opschotdetectie_voorbeeld.html#uitknippen-van-opschot-met-segmentanything",
    "href": "examples/opschotdetectie_voorbeeld.html#uitknippen-van-opschot-met-segmentanything",
    "title": "Opschotdetectie",
    "section": "Uitknippen van opschot met SegmentAnything",
    "text": "Uitknippen van opschot met SegmentAnything\nOp basis van de zojuist gedetecteerde objecten, kunnen met SegmentAnything de contouren gedetecteerd worden.\n\n\nCode\ndef show_mask(\n    mask: np.ndarray, ax: matplotlib.axes.Axes, random_color: bool = False\n) -&gt; None:\n    if random_color:\n        color = np.random.rand(3)  # Generates three random floats between 0 and 1\n        color = np.append(color, 0.6)  # Add alpha for transparency\n    else:\n        color = np.array(\n            [30 / 255, 144 / 255, 255 / 255, 0.6]\n        )  # Deep sky blue with transparency\n\n    h, w = mask.shape\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    return None\n\n# Segment objects with SAM\nsam_predictor.set_image(image_np)\ntransformed_boxes = sam_predictor.transform.apply_boxes_torch(\n    boxes_filt_clean, image_np.shape[:2]\n).to(\"cuda:1\")\nmasks, _, _ = sam_predictor.predict_torch(\n    point_coords=None,\n    point_labels=None,\n    boxes=transformed_boxes.to(\"cuda:1\"),\n    multimask_output=False,\n)\n\nfor cat_title, mask in zip(pred_phrases_clean, masks):\n    mask = mask.cpu().numpy()\n\n# Setup figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=100, squeeze=False)\n\nax = axs[0, 0]\nax.imshow(image_np)\nax.set_title(\"Origineel\", wrap=True)\nax.axis(\"off\");\n\nax = axs[0, 1]\nax.imshow(image_np)\nfor mask in masks:\n    show_mask(mask[0,...].cpu().numpy(), ax, random_color=False)\nfor box, label in zip(boxes_filt_clean, pred_phrases_clean):\n    show_box(box.numpy(), ax, label)\nax.set_title(f\"GroundingDino + SegmentAnything, tags: {tag_list}\", wrap=True)\nax.axis(\"off\")\nfig.tight_layout();",
    "crumbs": [
      "Meer weten",
      "Meer over de AI Toolbox",
      "Opschotdetectie"
    ]
  }
]